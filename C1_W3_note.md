+ lab_utils_common.py:
  + sigmoid(z) :
    + 逻辑解析：
      + np.clip(z, -500, 500): 将输入z中的值限制在区间[-500,500] 内，这是为了防止出现数值溢出的情况，因为在计算sigmoid函数时，如果输入值过大或过小，可能会导致指数函数计算结果溢出，从而导致错误或不稳定的计算结果
      + g 来定义sigmoid函数的式子（见笔记）
  + predict_logistic：（获得逻辑回归函数）
    + 由于sigmoid 函数已经定义了g(z) 的形式
    + 于是本函数将 参数z 替换
    + 得到逻辑回归函数的完整形式，其中参数为X，w，b
  + predict_liner：（获得线性回归函数）
    + X @ w + b
    + 获得经典的线性回归函数，目的是与逻辑回归函数进行对比
  + compute_cost_logistic：计算逻辑回归的代价函数
    + 参数分析：
      + X：实例训练集x，大小为（m，n），表示有m个实例，每个实例有n个特征
      + y：实例训练集y，大小为（m，），表示有m个实例y，与m个实例x相对应
      + w：权重参数，大小为（n，），与每个实例x都有n个特征相对应
      + b：偏置参数，标量
      + lambda_: 标量，用于控制正则化的强度，默认为0，表示不进行正则化
      + safe：布尔值，用于选择是否采用安全的算法来避免数值溢出，默认为False
    + 逻辑解析：
      + 获取矩阵X的形状，m表示样本数量，n表示特征数量，并初始化cost
      + 对每个样本先进行线性模型的形式： z = X*w + b
      + 如果safe参数为True，则采用防止数值溢出的安全算法，计算损失值。
        + 其实也就是在把式子带进去化简了，避免求对数的时候，参数的值过于小，导致趋于负无穷了
      + 如果safe参数为False：则直接计算损失值，
      + 计算平均损失值
      + 初始化正则化损失值 reg_cost 为0
      + 如果正则化lambda_ 不为0，则进行正则化
      + 最终损失值： cost + reg_cost
  + log_lpexp():
    + 逻辑解析：
      + out : 创建一个和输入x 同样形状的全零数组，用来存储出结果，指定数据类型为float
      + i = x <= maximum : 创建一个布尔型数组i，其元素为True或False ，表示对应位置上的x是否小于等于maximum。这里判断了输入数组x中的每个元素是否小于等于 maximum
        + 布尔索引：
          + 生成一个布尔数组，用来表示数组x中的元素是否满足某个条件
      + ni = np.logical_not(i) : 创建一个和i形状相同的布尔型数组ni，其元素为i对应位置上元素的逻辑非（取反）。这里ni 表示x中的每个元素是否大于maximum
      + out[i] = np.log(1 + np.exp(x[i]) ) : 对于输入数组x 中小于等于maximum 的元素，计算近似的对数值，这个近似的对数值计算是为了避免在计算过程中发生数值溢出的问题，尤其是对于较大的输入值
        + 布尔型数组的索引：
          + 在Python中，可以通过布尔型数组来进行数组索引操作，用来筛选数组中满足某个条件的元素。
          + 在这个语境下，i中的True 表示对应位置的元素满足条件，False 表示不满足条件
      + out[ni] = x[ni] : 对于输入数组x中大于maximum的元素，直接将其复制到输出数组out中。 这是因为在这种情况下，exp(x) 的值已经非常大，不需要进行相似的对数值计算
      + 输出数组out
  + compute_cost_matrix: 
    + 逻辑解析：
      + 获取输入数据的样本数量：m
      + 将目标值y和参数w转换为二维数组形式，确保其维度
      + 如果logistic 参数为True，表示我们正在进行逻辑回归的计算
        + 如果safe 为True，表示我们希望避免溢出的情况（即安全运算）
        + 如果safe为False，则采用常规的计算方式，并将损失值转换为标量形式（cost[0,0]）
      + 如果logistic 参数为False，表示我们正在进行线性回归的计算
        + 计算线性回归模型的输出，计算其均方差损失
      + 计算正则化项的损失
      + 将总损失定义为预测损失和正则化损失之和，并返回总损失
  + compute_gradient_matrix :
    + 逻辑解析：
      + 获取样本数量m
      + 将y，w转换为二维数组（确保形状为（m，1））
      + 计算模型的预测值，如果 logistic参数为True，则启动sigmoid激活函数，否则就直接返回线性加权和
      + 计算预测值和真实值之间的误差err
      + 计算损失函数关于参数w的梯度，这是通过将误差与输入特征矩阵X进行乘法运算，然后除以样本数量来实现的 ，这里使用了矩阵乘法
      + 计算损失函数关于参数b的梯度
      + 如果lambda_ 参数不是0，则应用正则化，这里更新了关于参数w的梯度，以惩罚较大的梯度
      + 最后返回参数b和w的梯度值
  + gradient_descent : 
    + 逻辑解析：
      + 创建了一个空列表 J_history，用于存储每次迭代后的损失函数值
      + 深度拷贝创建参数w的副本，以避免在函数内部修改全局参数w
      + 将参数w 和目标值y 转换为二维数组的列向量（-1，1） ，以便进行矩阵运算
      + 开始进行梯度下降的迭代，循环次数为num_iters
        + 调用 compute_gradient_matrix 计算损失函数关于参数w和b的梯度
        + 使用梯度下降更新参数w，b
        + 计算当前迭代后的损失函数值，并将其添加到损失值历史列表中
        + 每隔一定的迭代次数就打印一次损失值
      + 返回更新后的参数w，b，以及损失值历史列表J_histroy
  + zscore_normalize_features:
    + 逻辑解析：
      + 计算输入数据X 每一列的均值，结果返回一个形状为（n，）的一维数组，其中n是特征的数量
      + 计算输入数据X每一列的标准差，同样是沿着列的方向进行计算，结果是一个形状为（n，）的以为数组，n是特征数量
      + 将输入数据X中的每个元素减去对应列的均值mu，并处以对应列的标准差sigma，以对每一列进行 Z 分归一化处理
      + 返回归一化后的输入数据X，以及每列的均值mu和标准差sigma
  
  + 绘图 ：
  + plot_data：
    + 逻辑解析：
      + pos，neg这两行代码用于找到正例和负例的索引：
        + 如果y中的元素等于 1，则pos中对应的位置为True，False，同理，如果y中的元素等于0，则neg中的对应位置为True， 否则为False
      + pos，neg 重新调整为形状为一维数组，这样做是为了确保与输入数据的维度匹配，以便后续绘图的时候使用
      + 绘制正例的散点图，X[pos, 0],X[pos, 1] 表示X中所有正例对应的第一个和第二个特征
      + 绘制负例的散点图，参数设置与绘制正例的散点图类似
      + 显示图例
      + 隐藏图形的工具栏，标题，和页脚，以改善图形的显示效果
  + plt_tumor_data:
    + 依然是找到正例和负例的索引，如果y中的元素等于1，pos中对应的位置为True，否则为False。同理如果y中的元素等于0，则neg中对应的位置为True，否则为False
    + 绘制恶性肿瘤的散点图
    + 绘制良性肿瘤的散点图
  + draw_vthresh:
    + 作用：
      + 绘制阈值
    + 逻辑解析：
      + 获取y，x轴范围
      + 在x轴范围内，从左边界到阈值x的区域填充色块
      + 在x轴范围内，从阈值x 到右边界的区域填充色块
      + 在坐标（x，0。5）处添加文本
      + 创建一个箭头表示z >= 0的情况
      + 将箭头添加到图形中
